import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from scipy.spatial.distance import jensenshannon
from scipy.stats import pearsonr

from dython.nominal import compute_associations

from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, OneHotEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, plot_confusion_matrix, classification_report
from sklearn.compose import ColumnTransformer
from sklearn.inspection import permutation_importance

from imblearn.pipeline import Pipeline

import tempfile

from spmf import Spmf

import sys
import os
import math

class HiddenPrints:
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout

def _is_sequence(sequence):
        return (len(sequence)>=2) or (' ' in sequence[0])

class QualityEvaluator:
    """
    Evaluation of synthetic patient and treatment data, including static patient covariates and treatments
    
    Input:
    * original_train_static (pd.DataFrame): original patient covariates used for training (generating) synthetic data 
    * original_train_treatments (pd.DataFrame): original treatments used for training (generating) synthetic data
    * original_test_static (pd.DataFrame): original holdout set of patient covariates
    * original_test_treatments (pd.DataFrame): original holdout set of treatments
    * maingroup_dict (dict): translation dictionary from treatment codes to main groups
    * synth_static (dict): dictionary of patient covariates generated by different generative methods. 
        In form {generator_name (str): generated_patient_covariates (pd.DataFrame)}
    * synth_treatments: dictionary of treatments generated by different generative methods.
        In form {generator_name (str): generated_patient_covariates (pd.DataFrame)}
        Note that generator_name should match names as in synth_static
    * feature_rename_dict (optional) (dict): dictionary to provide feature names for labeled figures
    """
    def __init__(self, original_train_static, original_train_treatments, original_test_static, original_test_treatments,
                 maingroup_dict, synth_static, synth_treatments, feature_rename_dict=None):
        self.original_train_static = original_train_static
        self.original_train_treatments = original_train_treatments
        self.original_test_static = original_test_static
        self.original_test_treatments = original_test_treatments
        self.maingroup_dict = maingroup_dict
        self.synth_static = synth_static
        self.synth_treatments = synth_treatments
        self.feature_rename_dict = feature_rename_dict
    
    @staticmethod
    def _jensenshannon_pairwise(df_original, df_synth):
        """
        Jensen-Shannon distance over all features in dataframe
        Returns average distance
        
        Parameters:
        * df_original: original data frame used for training
        * df_synth: synthetic data frame generated 
        """
        assert (df_original.columns == df_synth.columns).all(), "input df_original and df_synth have different features"
        feature_distances = {}
        average_feature_distance = np.empty_like(df_original.columns)
        for i, c in enumerate(df_original.columns):
            counts_df_original, counts_df_synth = df_original[c].value_counts(dropna=False).align(df_synth[c].value_counts(
                dropna=False), join='outer', axis=0, fill_value=0)
            js_distance = jensenshannon(counts_df_original, counts_df_synth)
            average_feature_distance[i] = js_distance
            feature_distances[c] = js_distance
        average_feature_distance = np.sum(average_feature_distance) / len(df_original.columns)
        return average_feature_distance
    
    def m1_JSdistance_covariates(self, visual=False, visual_per_covariate=False):
        """
        Computes average Jensen-Shannon distance over all patient covariates for all synthetic data sets compared to
        original training data frame
        
        Parameters:
        * visual (optional) (bool): if True, show combined plot of comparison of probability distribution over all features
        * visual_per_covariate (optional) (bool): if True, show plots of comparison of probability distribution per feature
        """
        js_distances = {}
        
        for synth_name, df_synth in self.synth_static.items():
            js_distance = self._jensenshannon_pairwise(self.original_train_static, df_synth)
            js_distances[synth_name] = js_distance
            
        if visual or visual_per_covariate:
            
            vc = {'original': {}}
            vc_original = {}
            
            # Original
            for idx, col in enumerate(self.original_train_static.columns):
                vc_col = self.original_train_static[col].value_counts(normalize=True).sort_index()
                vc_col.name = 'original'
                
                vc_original[col] = vc_col
                            
                vc_col_dict = dict(zip([col + '__' + idx for idx in vc_col.index], vc_col.values))
                vc['original'].update(vc_col_dict)
                
            # Order values in vc based on probability (value)
            sorted_values = sorted([(value, key) for key, value in vc['original'].items()])
            vc['original'] = {key: value for value, key in sorted_values}
                
            # Synthetic
            for synth_name, df_synth in self.synth_static.items():
                vc[synth_name] = {}
                for idx, col in enumerate(self.original_train_static.columns):
                    vc_col = df_synth[col].value_counts(normalize=True).sort_index()
                    vc_col.name = 'synthetic'
                    
                    # Merge with real in case of missing categories
                    combined = pd.DataFrame(vc_original[col]).merge(pd.DataFrame(vc_col), 
                                                                    left_index=True, right_index=True).fillna(0)
                    vc_col = combined['synthetic']
                    vc_col_dict = dict(zip([col + '__' + idx for idx in vc_col.index], vc_col.values))
                    vc[synth_name].update(vc_col_dict)
                
                # Same order as original
                vc[synth_name] = {key: vc[synth_name][key] for key in vc['original'].keys()}
                    
            if visual: 
                fig, ax = plt.subplots(1, len(self.synth_treatments), figsize=(len(self.synth_static)*4, 4), sharey=True)
                # Plot
                for i, synth_name in enumerate(self.synth_treatments.keys()):
                    # Plot values
                    ax[i].bar(x=list(vc[synth_name].keys()), height=list(vc[synth_name].values()),
                              color='cornflowerblue', label=synth_name) 
                    # Plot comparison with real
                    ax[i].plot(list(vc['original'].keys()), list(vc['original'].values()),
                                    color='#A9D18E', linewidth=2, label='original')
                    # Layout
                    ax[i].axes.xaxis.set_ticks([]) # No x labels
                    ax[i].set_title(synth_name + '\n JS distance: {}'.format(round(js_distances[synth_name], 4)))
                    ax[i].set_xlabel('Patient covariates')
                    ax[i].set_ylabel('Probability')
                    ax[i].legend()
                    
                    ax[i].yaxis.set_tick_params(labelleft=True)
            
            if visual_per_covariate:
                
                fig, ax = plt.subplots(len(self.original_train_static.columns), len(self.synth_treatments), figsize=(20, 32), sharey='row')
                
                for i, synth_name in enumerate(self.synth_treatments.keys()):
                    for j, col in enumerate(self.original_train_static.columns):
                        col_values_synth = {key[len(col)+2:]: value for key, value in vc[synth_name].items() if 
                                            key.startswith(col)}
                        col_values_original = {key[len(col)+2:]: value for key, value in vc['original'].items() if 
                                               key.startswith(col)}
                        
                        # Sort on category by name
                        col_values_synth = {key: col_values_synth[key] for key in sorted(list(col_values_original.keys()))}
                        col_values_original = {key: col_values_original[key] for key in sorted(list(col_values_original.keys()))}
                        
                        # Plot values for column
                        ax[j][i].bar(x=list(col_values_synth.keys()), height=list(col_values_synth.values()),
                                     color='cornflowerblue', label=synth_name) 
                        # Plot comparison with real
                        ax[j][i].plot(list(col_values_original.keys()), 
                                      list(col_values_original.values()), marker='o', markersize=5,
                                      color='#A9D18E', linewidth=2, label='original')

                        # Layout
                        if j==0:
                            ax[j][i].set_title(synth_name + '\n', fontweight='bold')
                        if self.feature_rename_dict:
                            ax[j][i].set_xlabel(self.feature_rename_dict[col])
                        else:
                            ax[j][i].set_xlabel(col)
                        ax[j][i].set_ylabel('Probability')
                        ax[j][i].legend()
                        
                        ax[j][i].yaxis.set_tick_params(labelleft=True)
                
            plt.tight_layout()

        return js_distances
    
    @staticmethod
    def _get_treatment_occurrence(df, synth_real):
        """
        Computes treatment occurrence percentage in data set
        """
        occurrence_percentages = pd.DataFrame(df['gbs_gebeurtenis_code'].value_counts(normalize=True)).rename(
            columns={'gbs_gebeurtenis_code': synth_real})
        
        return occurrence_percentages
    
    def m2_RMSE_treatment_occurrences(self, visual=False, metric='RMSE'):
        """
        Computes RMSE (or CC) treatment occurrence percentages between original training data set for all synthetic data sets
        
        Parameters:
        * visual (bool): if True, show scatter plots of treatment occurrence percentages
        """
        occ_perc_original = self._get_treatment_occurrence(self.original_train_treatments, synth_real='original')
        
        occ_perc_dict = {}
        if metric=='RMSE':
            rmse = {}
        else:
            cc = {}
        for synth_name, df_synth in self.synth_treatments.items():
            occ_perc_synth = self._get_treatment_occurrence(df_synth, synth_real='synthetic')
            
            # Merge with occ_perc_original in case treatments are not present in synthetic data set
            occ_perc = occ_perc_original.merge(occ_perc_synth,
                                               left_index=True,
                                               right_index=True,
                                               how='left').fillna(0)
            occ_perc_dict[synth_name] = occ_perc
            
            # RMSE
            if metric=='RMSE':
                rmse_occ = mean_squared_error(occ_perc['original'], occ_perc['synthetic'], squared=False)
                rmse[synth_name] = rmse_occ
            else:
                cc_occ = pearsonr(occ_perc['original'], occ_perc['synthetic'])[0]
                cc[synth_name] = cc_occ
                
            
        if visual:
            fig, ax = plt.subplots(1, len(self.synth_treatments), figsize=(20, 10))
        
            for i, synth_name in enumerate(self.synth_treatments.keys()):        
                # Subplot
                ax[i].scatter(occ_perc_dict[synth_name]['original'], occ_perc_dict[synth_name]['synthetic'], s=15, 
                              color='cornflowerblue')
                ax[i].plot(occ_perc_dict[synth_name]['original'], occ_perc_dict[synth_name]['original'], color='black')
                ax[i].set_xlabel('Original')
                ax[i].set_ylabel(synth_name)
                ax[i].set_title(synth_name)
                ax[i].set_aspect('equal')
                
                # RMSE text
                if metric=='RMSE':
                    textstr = 'RMSE: {}'.format(round(rmse[synth_name], 4))
                else:
                    textstr = 'CC: {}'.format(round(cc[synth_name], 4))
                props = dict(boxstyle='square', facecolor='none', alpha=0.5)
                ax[i].text(0.05, 0.95, textstr, transform=ax[i].transAxes, fontsize=12, verticalalignment='top', bbox=props)
                
                plt.tight_layout()
                
        if metric=='RMSE':
            return rmse
        else:
            return cc
        
    def m3_SC_treatments(self):
        """
        Computes Support Coverage of treatment codes for all synthetic data sets
        """
        support_coverages = {}
        
        occ_perc_original = self._get_treatment_occurrence(self.original_train_treatments, synth_real='original')
        
        for synth_name, df_synth in self.synth_treatments.items():
            occ_perc_synth = self._get_treatment_occurrence(df_synth, synth_real='synthetic')
            support_coverages[synth_name] = len(occ_perc_synth)/len(occ_perc_original)
        
        return support_coverages
    
    def m4_JSdistance_sequence_lengths(self, visual=False):
        """
        Computes Jensen-Shannon distance between original training sequence length distributions for all synthetic data sets
        
        Parameters
        * visual (bool): if True, show bar plots of sequence length distributions
        """
        sequence_length_original = self.original_train_treatments.groupby('eid')['gbs_gebeurtenis_code'].count()
        
        js_distances = {}
        
        for synth_name, df_synth in self.synth_treatments.items():
            sequence_length_synth = df_synth.groupby('eid')['gbs_gebeurtenis_code'].count()
            
            # Counts
            counts_df_original, counts_df_synth = sequence_length_original.value_counts(dropna=False).align(
                sequence_length_synth.value_counts(dropna=False), join='outer', axis=0, fill_value=0)
            js_distance = jensenshannon(counts_df_original, counts_df_synth)
            js_distances[synth_name] = js_distance
            
        if visual:
            probabilities_original = pd.DataFrame(sequence_length_original.value_counts(normalize=True)).rename(
                    {'gbs_gebeurtenis_code': 'original'}, axis=1)
            
            fig, ax = plt.subplots(1, len(self.synth_treatments), figsize=(20, 20), sharey=True)
            
            for i, synth_name in enumerate(self.synth_treatments.keys()):
                sequence_length_synth = self.synth_treatments[synth_name].groupby('eid')['gbs_gebeurtenis_code'].count()
                probabilities_synth = pd.DataFrame(sequence_length_synth.value_counts(normalize=True)).rename(
                    {'gbs_gebeurtenis_code': synth_name}, axis=1)
                
                probabilities = probabilities_original.merge(probabilities_synth, 
                                                             how='outer', 
                                                             left_index=True, 
                                                             right_index=True)

                probabilities.plot(kind='bar', ax=ax[i], color=['cornflowerblue', '#A9D18E'])
                ax[i].set_xlabel('Sequence length ($|\mathcal{S}_p|$)')
                ax[i].set_ylabel('Probability')
                ax[i].set_aspect(8)
                ax[i].set_title(synth_name + '\n' + 'JS distance: {}'.format(round(js_distances[synth_name], 4)))
        
                ax[i].yaxis.set_tick_params(labelleft=True)
                
                plt.tight_layout()
        
        return js_distances
    
    @staticmethod
    def _get_hoofdgroepen(treatments_list, lookup_dict):
        """
        Transforms list of treatment codes to list of treatment main groups
        
        Parameters:
        * treatments_list (list) = List of treatments for a patient
        * lookup_dict (dict) = translation dictionary with specific treatment code as keys and main group as values
        """
        return [lookup_dict[treat] for treat in treatments_list]
        
    def _get_extended_dataset_V(self, df_static, df_treatments, mlb=None):
        """
        Function that returns extended static version V of data set 
        V is a concatenation of patient covariates, binarized treatment vector per main group and sequence length for each patient
        Returns extended data set V and mlb (for internal use only)
        
        Parameters:
        * df_static: patient covariates part of data set
        * df_treatments: treatments part of data set
        * mlb (optional, for internal use only): uses mlb of original data set for synthetic data set
        """
        # Get ordered treatments list per patient
        treatments_per_patient = pd.DataFrame(df_treatments.groupby('eid')['gbs_gebeurtenis_code'].apply(list))
        # Assign each treatment to its main group
        main_groups = pd.DataFrame(treatments_per_patient['gbs_gebeurtenis_code'].apply(self._get_hoofdgroepen, 
                                                                                        args=[self.maingroup_dict]))
        
        # Binarize
        if not mlb:
            mlb = MultiLabelBinarizer()
            mlb.fit(main_groups['gbs_gebeurtenis_code'])
        
        binarized_treatment_vector = pd.DataFrame(mlb.transform(main_groups['gbs_gebeurtenis_code']),
                                                  columns=mlb.classes_,
                                                  index=main_groups.index)
        
        extended_V = df_static.merge(binarized_treatment_vector, left_index=True, right_index=True)
        
        # Get sequence length per patient
        sequence_lengths = pd.DataFrame(df_treatments.groupby('eid')['gbs_gebeurtenis_code'].count())
        
        extended_V = extended_V.merge(sequence_lengths, left_index=True, right_index=True)
        
        # Change gender, if needed
        extended_V['pat_geslacht_code'] = extended_V['pat_geslacht_code'].replace({'Female': 0, 'Male': 1})
        extended_V['survival_1'] = extended_V['survival_1'].astype(int)
        
        # Change feature names
        if self.feature_rename_dict:
            extended_V = extended_V.rename(self.feature_rename_dict, axis=1)

        return extended_V, mlb
        
    def m5_PCD(self, nominal_columns, visual=False, visual_difference=False, visual_per_synth_dataset=False, fill_diagonal=False):
        """
        Computes pairwise correlation difference (PCD) between original association matrix of extended data set V
        for all synthetic data sets
        
        Parameters:
        * nominal_columns (list): list of nominal columns in data frame
        * visual (optional) (bool): if True, returns small-sized assocation matrices (heatmaps) for all synthetic data sets
             and original data set
        * visual_difference (optional) (bool): if True, returns heatmaps of differences in assocation values with original data set
            for all synthetic data sets
        * visual_per_synth_dataset (optional) (bool): if True, return bigger-sized assocation matrices (heatmaps) for all synthetic
            data sets
        * fill_diagonal (optional) (bool): fill the diagonal of all association matrices with 1's
        """
        # Get extended original data set
        V_original, mlb = self._get_extended_dataset_V(self.original_train_static, self.original_train_treatments)
        
        # Get associations of original extended data set
        ass_original = compute_associations(V_original, theil_u=True, nominal_columns=nominal_columns)
        
        ass_synth = {}
        pcd = {}
        for synth_name in self.synth_static.keys():
            # Get extended synthetic data set
            V_synth, _ = self._get_extended_dataset_V(self.synth_static[synth_name], self.synth_treatments[synth_name], mlb)
            # Get associations of synthetic extended data set
            ass = compute_associations(V_synth, theil_u=True, nominal_columns=nominal_columns)
            
            if fill_diagonal:
                np.fill_diagonal(ass.values, 1.0)
            
            ass_synth[synth_name] = ass
                        
            # Compute PCD
            pcd[synth_name] = {}
            
            static_col_len = len(self.original_train_static.columns)
                
            pcd[synth_name]['total'] = np.linalg.norm(ass_original-ass, 'fro')
            
            pcd[synth_name]['static'] = np.linalg.norm(ass_original.iloc[:static_col_len, :static_col_len]-
                                                       ass.iloc[:static_col_len, :static_col_len], 'fro')
            
            
            pcd[synth_name]['static_treatments'] = np.linalg.norm(ass_original.iloc[:static_col_len, static_col_len:]-
                                                                  ass.iloc[:static_col_len, static_col_len:], 'fro')
            
            
            pcd[synth_name]['treatments'] = np.linalg.norm(ass_original.iloc[static_col_len:, static_col_len:]-
                                                           ass.iloc[static_col_len:, static_col_len:], 'fro')
            
        if visual:
            fig, ax = plt.subplots(1, len(ass_synth)+1, figsize=((len(ass_synth)+2)*2, 4))
            cbar_ax = fig.add_axes([.91, 0.3, .01, .4])
            
            cmap = sns.diverging_palette(220, 10, as_cmap=True)
            
            # Take minimum and maximum association value in original data set for consistency across visualizations
            vmin = math.floor(ass_original.values.min()*10)/10
            vmax = math.ceil(ass_original.values.max()*10)/10
            
            # Synthetic 
            for i, synth_name in enumerate(self.synth_treatments.keys()):
                az = sns.heatmap(ass_synth[synth_name], ax=ax[i], square=True, annot=False, center=0, linewidths=0, 
                                 cmap=cmap, xticklabels=False, yticklabels=False, cbar=False, cbar_kws={'shrink': 0.8},
                                 vmin=vmin, vmax=vmax)
                ax[i].set_title(synth_name + '\n' + 'PCD: {}'.format(round(pcd[synth_name]['total'], 4)))
            
            # Original
            az = sns.heatmap(ass_original, ax=ax[-1], square=True, annot=False, center=0, linewidths=0, 
                                     cmap=cmap, xticklabels=False, yticklabels=False, cbar_kws={'shrink': 0.8}, 
                                     cbar_ax=cbar_ax, fmt='.2f', vmin=vmin, vmax=vmax)
            ax[-1].set_title('Original \n')
            
            cbar = az.collections[0].colorbar
            cbar.ax.tick_params(labelsize=10)
            
        if visual_difference:
            fig, ax = plt.subplots(1, len(ass_synth)+1, figsize=((len(ass_synth)+2)*2, 4))
            cbar_ax = fig.add_axes([.91, 0.3, .01, .4])
                        
            # Take minimum and maximum association value in original data set for consistency across visualizations
            vmin = math.floor(ass_original.values.min()*10)/10
            vmax = math.ceil(ass_original.values.max()*10)/10
            
            # Synthetic 
            for i, synth_name in enumerate(self.synth_treatments.keys()):
                az = sns.heatmap(ass_original-ass_synth[synth_name], ax=ax[i], square=True, annot=False, center=0, linewidths=0, 
                                 cmap='PRGn', xticklabels=False, yticklabels=False, cbar=False, cbar_kws={'shrink': 0.8},
                                 vmin=vmin, vmax=vmax)
                ax[i].set_title(synth_name + '\n' + 'PCD: {}'.format(round(pcd[synth_name]['total'], 4)))
            
            # Original --> just for cbar purposes
            az = sns.heatmap(ass_original-ass_original, ax=ax[-1], square=True, annot=False, center=0, linewidths=0, 
                                     cmap='PRGn', xticklabels=False, yticklabels=False, cbar_kws={'shrink': 0.8}, 
                                     cbar_ax=cbar_ax, fmt='.2f', vmin=vmin, vmax=vmax)
            
            cbar = az.collections[0].colorbar
            cbar.ax.tick_params(labelsize=10)
            
        if visual_per_synth_dataset:
            for i, synth_name in enumerate(self.synth_treatments.keys()):    
                cmap = sns.diverging_palette(220, 10, as_cmap=True)
                
                # Take minimum and maximum association value in original data set for consistency across visualizations
                vmin = math.floor(ass_original.values.min()*10)/10
                vmax = math.ceil(ass_original.values.max()*10)/10

                plt.figure(figsize = (15, 15))
                sns.heatmap(round(ass_synth[synth_name], 2), square=True, annot=True, center=0, linewidths=0, cmap=cmap, 
                            xticklabels=True, yticklabels=True, cbar_kws={'shrink': 0.8}, vmin=vmin, vmax=vmax)
                plt.title(synth_name + '\n' + 'PCD: {}'.format(round(pcd[synth_name]['total'], 4)))
                plt.show()
                
        return pcd
    
    def m6_TBTOH(self, feature_importances=False, n_jobs=1, visual=False):
        """
        Trains and optimizes RF 1-year survival prediction model on extended versions of original train and all
        synthetic data sets
        Computes ratio of AUC-ROC score of model trained on original train data set and tested on original test
        data set and model trained on synthetic data set and tested on original test data set for all synthetic data sets
        
        Parameters:
        * feature_importances (optional) (bool): if True, compute and store permutation feature importance scores
        * n_jobs (optional) (int): maximum number of concurrently running workers by GridSearch (default=1)
        * visual (optional) (bool): if True, show bar plot of AUC-ROC scores of generative models with line indicating AUC-ROC score
            of original training set
        """
        # Get extended original train and test (hold out) data sets
        V_original_train, mlb = self._get_extended_dataset_V(self.original_train_static, self.original_train_treatments)
        V_original_test, _ = self._get_extended_dataset_V(self.original_test_static, self.original_test_treatments, mlb)
        
        ## Original
        # Set survival_1 to death_1 
        V_original_train['death_1'] = (V_original_train['1 year survival'] - 1) * -1 # Train
        V_original_test['death_1'] = (V_original_test['1 year survival'] - 1) * -1 # Test
        
        V_original_train = V_original_train.drop('1 year survival', axis=1)
        V_original_test = V_original_test.drop('1 year survival', axis=1)
        
        # Set dtype gender to str
        V_original_train['Gender'] = V_original_train['Gender'].astype(str)
        V_original_test['Gender'] = V_original_test['Gender'].astype(str)
        
        # X and y
        X_original_train = V_original_train.drop('death_1', axis=1).copy()
        y_original_train = V_original_train['death_1'].copy()

        X_original_test = V_original_test.drop('death_1', axis=1).copy()
        y_original_test = V_original_test['death_1'].copy() 

        # Train
        # Preprocessing
        categorical_features = X_original_train.select_dtypes(include=['object']).columns
        numeric_features = [feat for feat in X_original_train.columns if not feat in categorical_features]

        preprocessor = ColumnTransformer(transformers=[
            ('num_scaling', MinMaxScaler(), numeric_features),
            ('categorical_encoding', OneHotEncoder(drop='if_binary'), categorical_features)])
        
        # Classifier
        clf_rf = RandomForestClassifier(class_weight='balanced', min_samples_leaf=0.05, random_state=2021)
        
        # Pipeline
        pipe = Pipeline([('preprocessor', preprocessor),
                         ('classify', clf_rf)])
        
        # Grid search
        params = {'classify__n_estimators': [100, 150, 200],
                  'classify__criterion': ['entropy', 'gini'],
                  'classify__max_depth': [3, 5, 10],
                  'classify__max_features': ['sqrt', 'log2']}
    
        grid_rf = GridSearchCV(pipe, 
                               param_grid=params,
                               scoring='roc_auc',
                               refit=True,
                               cv=5,
                               verbose=2,
                               n_jobs=n_jobs)
       
        # Fit on original train
        grid_rf.fit(X_original_train, y_original_train);

        # Test on original test
        roc_auc = roc_auc_score(y_original_test, grid_rf.predict_proba(X_original_test)[:, 1])
        
        if feature_importances:
            permutation_importances_rf = permutation_importance(grid_rf, X_original_test, y_original_test, scoring='roc_auc')
        
        # Dictionaries to store results
        auc = {'original': roc_auc} # AUC_ROC scores
        if feature_importances:
            self.rf_importances_ = {'original': permutation_importances_rf} 
        self.best_models_ = {'original': grid_rf.best_estimator_} # Best models
        
        print('Finished training model on original train set')
        
        ## Synthetic
        for synth_name in self.synth_static.keys():
            # Get extended synthetic data set
            V_synth, _ = self._get_extended_dataset_V(self.synth_static[synth_name], self.synth_treatments[synth_name], mlb)
            
            # Set survival_1 to death_1 
            V_synth['death_1'] = (V_synth['1 year survival'] - 1) * -1 # Train

            V_synth = V_synth.drop('1 year survival', axis=1)

            # Set dtype gender to str
            V_synth['Gender'] = V_synth['Gender'].astype(str)

            # X and y
            X_train = V_synth.drop('death_1', axis=1).copy()
            y_train = V_synth['death_1'].copy()
            
            # Train
            # Preprocessing
            categorical_features = X_train.select_dtypes(include=['object']).columns
            numeric_features = [feat for feat in X_train.columns if not feat in categorical_features]

            preprocessor = ColumnTransformer(transformers=[
                ('num_scaling', MinMaxScaler(), numeric_features),
                ('categorical_encoding', OneHotEncoder(drop='if_binary'), categorical_features)])

            # Classifier
            clf_rf = RandomForestClassifier(class_weight='balanced', min_samples_leaf=0.05, random_state=2021)

            # Pipeline
            pipe = Pipeline([('preprocessor', preprocessor),
                             ('classify', clf_rf)])

            # Grid search
            params = {'classify__n_estimators': [100, 150, 200],
                      'classify__criterion': ['entropy', 'gini'],
                      'classify__max_depth': [3, 5, 10],
                      'classify__max_features': ['sqrt', 'log2']}

            grid_rf = GridSearchCV(pipe, 
                                   param_grid=params,
                                   scoring='roc_auc',
                                   refit=True,
                                   cv=5,
                                   n_jobs=n_jobs)

            # Fit on synthetic train
            grid_rf.fit(X_train, y_train);

            # Test on original test
            roc_auc = roc_auc_score(y_original_test, grid_rf.predict_proba(X_original_test)[:, 1])
            auc[synth_name] = {'auc': roc_auc,
                               'TB_TOH': roc_auc/auc['original']}
            
            if feature_importances:
                permutation_importances_rf = permutation_importance(grid_rf, X_original_test, y_original_test, scoring='roc_auc')
                self.rf_importances_[synth_name] = permutation_importances_rf
            
            self.best_models_[synth_name] = grid_rf.best_estimator_
            
            print('Finished training model on', synth_name)
        
        if visual:
            auc_scores = [auc[alg]['auc'] for alg in list(auc.keys())[1:]]
            
            # Bar plot
            plt.figure(figsize=(20, 10), dpi=80)
            plt.bar(x=list(auc.keys())[1:], height=auc_scores, color='cornflowerblue')
            plt.axhline(auc['original'], color='#A9D18E', linestyle='dashed', linewidth=4, label='Original')
            plt.xticks(fontsize=20)
            plt.xlabel('Model', fontsize=25)
            plt.ylabel('AUC-ROC score', fontsize=25)
            plt.title('Train on Both, Test on Original Holdout set (TB-TOH)', fontsize=30)
            plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), fontsize=15)
            plt.show()
            
        return auc
    
    @staticmethod
    def _preprocess_SPM(df, dict_codes):
        """
        Preprocess dataframe for sequential pattern mining
        
        Parameters:
        * df (pd.DataFrame): df with treatment codes
        * dict_codes (dict): dictionary that translates treatment codes to integers, with as treatment code as keys and integer as values
        """
        # Filter only patients with sequential treatments, i.e., more than 1 treatment
        sequence_lengths = df.groupby('eid')['gbs_gebeurtenis_code'].count()
        seq_patients = sequence_lengths[sequence_lengths>1].index
        df = df[df['eid'].isin(seq_patients)].copy()

        # Replace omschrijving code by code interpretable by Spmf
        df['code_mining'] = df['gbs_gebeurtenis_code']
        df['code_mining'] = df['code_mining'].replace(dict_codes)

        # Sort values on date
        df = df.sort_values(['eid', 'gbs_vnr'])

        # Group by tumour and date
        df_treatments_per_tumour = pd.DataFrame(df.groupby(['eid'])['code_mining'].apply(
            list)).rename(columns={'code_mining': 'treatments'}).reset_index()
        
        df_treatments_per_tumour['treatments'] = df_treatments_per_tumour['treatments'].apply(lambda treatments_list: 
                                                 [[treat] for treat in treatments_list])

        return df_treatments_per_tumour
    
    @staticmethod
    def _write_to_string(df_treatments_per_tumour):
        """
        Writes treatments column of dataframe to string used in sequential pattern mining
        
        * df_treatments_per_tumour (pd.DataFrame): df outputted by _preprocess_SPM
        """
        seq_treatments = list(df_treatments_per_tumour['treatments'])

        text_string = ''
        for sequence in seq_treatments:
            for sub_seq in sequence:
                for item in sub_seq:
                    text_string += str(item) + ' '
                text_string += '-1 '
            text_string += '-2\n'

        return text_string 
    
    @staticmethod
    def _sequential_pattern_mining_df(text_string, spmf_bin_location_dir, support=0.01):
        """
        Executes sequential pattern mining algorithm for a data set
        
        Parameters:
        * text_string (str): dataframe for SPM as string, output of _write_to_string
        * spmf_bin_location_dir: folder location of Java directory for SPM
        * support: minimum support in SPM algorithm, default = 0.01
        """ 
        f = tempfile.NamedTemporaryFile(mode = 'w+', delete=False)
        f.write(text_string)
        f.close()

        outfile = tempfile.NamedTemporaryFile(mode = 'w+', delete=False)
        
        spmf = Spmf('SPADE',
                    input_filename=f.name,
                    spmf_bin_location_dir=spmf_bin_location_dir,
                    output_filename=outfile.name,
                    arguments=[support])

        spmf.run()

        os.remove(f.name)
        
        mined_sequences = spmf.to_pandas_dataframe()
        mined_sequences = mined_sequences[mined_sequences['pattern'].apply(_is_sequence)].copy()
        mined_sequences['pattern'] = [tuple(pattern) for pattern in mined_sequences['pattern']]

        outfile.close()
        os.remove(outfile.name)

        return mined_sequences
    
    def m7_jaccardsimilarity_SPM(self, spmf_bin_location_dir, support=0.01):
        """
        Extracts frequent sequential patterns from all (original and synthetic) data sets
        Computes Jaccard similarity of frequent sequential pattern set between original and all synthetic data sets
        
        Parameters:
        * spmf_bin_location_dir: folder location of Java directory for SPM
        * support: minimum support in SPM algorithm, default = 0.01
        """
        # Set same SPM codes (integers) for all treatment codes
        dict_codes = dict(zip(self.original_train_treatments['gbs_gebeurtenis_code'].unique(), 
                              range(1, self.original_train_treatments['gbs_gebeurtenis_code'].nunique() + 1)))
        
        # SPM original
        df_original = self._preprocess_SPM(self.original_train_treatments, dict_codes)
        text_string_original = self._write_to_string(df_original)
        with HiddenPrints():
            mined_sequences_original = self._sequential_pattern_mining_df(text_string_original, spmf_bin_location_dir, support)

        spm = {'original': {'# frequent sequential patterns': len(mined_sequences_original)}}
        print('Finished SPM original')
        
        # SPM synthetic
        for synth_name, df_synth in self.synth_treatments.items():
            df = self._preprocess_SPM(df_synth, dict_codes)
            text_string = self._write_to_string(df)
            with HiddenPrints():
                mined_sequences_synth = self._sequential_pattern_mining_df(text_string, spmf_bin_location_dir, support)
                        
            # Compute scores
            intersection = set(mined_sequences_original['pattern']).intersection(set(mined_sequences_synth['pattern']))
            union = set(mined_sequences_original['pattern']).union(set(mined_sequences_synth['pattern']))
            
            if len(mined_sequences_synth):
                jaccard = len(intersection)/len(union)

            else:
                jaccard = 0
                
            spm_result = {'# frequent sequential patterns': len(mined_sequences_synth),
                          'intersection size': len(intersection),
                          'union size': len(union),
                          'jaccard similarity': jaccard,
                         }
            
            spm[synth_name] = spm_result
            
            print('Finished SPM', synth_name)
            
        return spm        